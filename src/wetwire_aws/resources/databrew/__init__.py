"""
AWS DATABREW CloudFormation resources.

Generated:
  Source: CloudFormation Spec 2026.01.02
  Generator: 1.0.0
  Date: 2026-01-07T05:29:20Z

DO NOT EDIT - This file is generated by wetwire-aws codegen.
To regenerate: python -m wetwire_aws.codegen.generate
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, ClassVar

from wetwire_aws.base import (
    CloudFormationResource,
    PropertyType,
    PropertyTypeDescriptor,
    Tag,
)
from wetwire_aws.typing import DslValue


# Constants
# ============================================================


class AnalyticsMode:
    ENABLE = "ENABLE"
    DISABLE = "DISABLE"


class CompressionFormat:
    GZIP = "GZIP"
    LZ4 = "LZ4"
    SNAPPY = "SNAPPY"
    BZIP2 = "BZIP2"
    DEFLATE = "DEFLATE"
    LZO = "LZO"
    BROTLI = "BROTLI"
    ZSTD = "ZSTD"
    ZLIB = "ZLIB"


class DatabaseOutputMode:
    NEW_TABLE = "NEW_TABLE"


class EncryptionMode:
    SSE_KMS = "SSE-KMS"
    SSE_S3 = "SSE-S3"


class InputFormat:
    CSV = "CSV"
    JSON = "JSON"
    PARQUET = "PARQUET"
    EXCEL = "EXCEL"
    ORC = "ORC"


class JobRunState:
    STARTING = "STARTING"
    RUNNING = "RUNNING"
    STOPPING = "STOPPING"
    STOPPED = "STOPPED"
    SUCCEEDED = "SUCCEEDED"
    FAILED = "FAILED"
    TIMEOUT = "TIMEOUT"


class JobType:
    PROFILE = "PROFILE"
    RECIPE = "RECIPE"


class LogSubscription:
    ENABLE = "ENABLE"
    DISABLE = "DISABLE"


class Order:
    DESCENDING = "DESCENDING"
    ASCENDING = "ASCENDING"


class OrderedBy:
    LAST_MODIFIED_DATE = "LAST_MODIFIED_DATE"


class OutputFormat:
    CSV = "CSV"
    JSON = "JSON"
    PARQUET = "PARQUET"
    GLUEPARQUET = "GLUEPARQUET"
    AVRO = "AVRO"
    ORC = "ORC"
    XML = "XML"
    TABLEAUHYPER = "TABLEAUHYPER"


class ParameterType:
    DATETIME = "Datetime"
    NUMBER = "Number"
    STRING = "String"


class SampleMode:
    FULL_DATASET = "FULL_DATASET"
    CUSTOM_ROWS = "CUSTOM_ROWS"


class SampleType:
    FIRST_N = "FIRST_N"
    LAST_N = "LAST_N"
    RANDOM = "RANDOM"


class SessionStatus:
    ASSIGNED = "ASSIGNED"
    FAILED = "FAILED"
    INITIALIZING = "INITIALIZING"
    PROVISIONING = "PROVISIONING"
    READY = "READY"
    RECYCLING = "RECYCLING"
    ROTATING = "ROTATING"
    TERMINATED = "TERMINATED"
    TERMINATING = "TERMINATING"
    UPDATING = "UPDATING"


class Source:
    S3 = "S3"
    DATA_CATALOG = "DATA-CATALOG"
    DATABASE = "DATABASE"


class ThresholdType:
    GREATER_THAN_OR_EQUAL = "GREATER_THAN_OR_EQUAL"
    LESS_THAN_OR_EQUAL = "LESS_THAN_OR_EQUAL"
    GREATER_THAN = "GREATER_THAN"
    LESS_THAN = "LESS_THAN"


class ThresholdUnit:
    COUNT = "COUNT"
    PERCENTAGE = "PERCENTAGE"


class ValidationMode:
    CHECK_ALL = "CHECK_ALL"


# PropertyType submodules
from . import Dataset as _Dataset
from . import Job as _Job
from . import Project as _Project
from . import Recipe as _Recipe
from . import Ruleset as _Ruleset


# Resources
# ============================================================


@dataclass
class Dataset(CloudFormationResource):
    """Dataset resource.

    http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-dataset.html

    CloudFormation type: AWS::DataBrew::Dataset
    """

    _resource_type: ClassVar[str] = "AWS::DataBrew::Dataset"

    input: DslValue[_Dataset.Input] | None = None
    name: DslValue[str] | None = None
    format: DslValue[str] | None = None
    format_options: DslValue[_Dataset.FormatOptions] | None = None
    path_options: DslValue[_Dataset.PathOptions] | None = None
    source: DslValue[str] | None = None
    tags: list[DslValue[Tag]] = field(default_factory=list)

    # PropertyType descriptors for nested GetAtt support
    CsvOptions = PropertyTypeDescriptor(_Dataset.CsvOptions, "CsvOptions")
    DataCatalogInputDefinition = PropertyTypeDescriptor(
        _Dataset.DataCatalogInputDefinition, "DataCatalogInputDefinition"
    )
    DatabaseInputDefinition = PropertyTypeDescriptor(
        _Dataset.DatabaseInputDefinition, "DatabaseInputDefinition"
    )
    DatasetParameter = PropertyTypeDescriptor(
        _Dataset.DatasetParameter, "DatasetParameter"
    )
    DatetimeOptions = PropertyTypeDescriptor(
        _Dataset.DatetimeOptions, "DatetimeOptions"
    )
    ExcelOptions = PropertyTypeDescriptor(_Dataset.ExcelOptions, "ExcelOptions")
    FilesLimit = PropertyTypeDescriptor(_Dataset.FilesLimit, "FilesLimit")
    FilterExpression = PropertyTypeDescriptor(
        _Dataset.FilterExpression, "FilterExpression"
    )
    FilterValue = PropertyTypeDescriptor(_Dataset.FilterValue, "FilterValue")
    FormatOptions = PropertyTypeDescriptor(_Dataset.FormatOptions, "FormatOptions")
    Input = PropertyTypeDescriptor(_Dataset.Input, "Input")
    JsonOptions = PropertyTypeDescriptor(_Dataset.JsonOptions, "JsonOptions")
    Metadata = PropertyTypeDescriptor(_Dataset.Metadata, "Metadata")
    PathOptions = PropertyTypeDescriptor(_Dataset.PathOptions, "PathOptions")
    PathParameter = PropertyTypeDescriptor(_Dataset.PathParameter, "PathParameter")
    S3Location = PropertyTypeDescriptor(_Dataset.S3Location, "S3Location")


@dataclass
class Job(CloudFormationResource):
    """Job resource.

    http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-job.html

    CloudFormation type: AWS::DataBrew::Job
    """

    _resource_type: ClassVar[str] = "AWS::DataBrew::Job"

    name: DslValue[str] | None = None
    role_arn: DslValue[str] | None = None
    type_: DslValue[str] | None = None
    data_catalog_outputs: list[DslValue[_Job.DataCatalogOutput]] = field(
        default_factory=list
    )
    database_outputs: list[DslValue[_Job.DatabaseOutput]] = field(default_factory=list)
    dataset_name: DslValue[str] | None = None
    encryption_key_arn: DslValue[str] | None = None
    encryption_mode: DslValue[str] | None = None
    job_sample: DslValue[_Job.JobSample] | None = None
    log_subscription: DslValue[str] | None = None
    max_capacity: DslValue[int] | None = None
    max_retries: DslValue[int] | None = None
    output_location: DslValue[_Job.OutputLocation] | None = None
    outputs: list[DslValue[_Job.Output]] = field(default_factory=list)
    profile_configuration: DslValue[_Job.ProfileConfiguration] | None = None
    project_name: DslValue[str] | None = None
    recipe: DslValue[_Job.Recipe] | None = None
    tags: list[DslValue[Tag]] = field(default_factory=list)
    timeout: DslValue[int] | None = None
    validation_configurations: list[DslValue[_Job.ValidationConfiguration]] = field(
        default_factory=list
    )

    # PropertyType descriptors for nested GetAtt support
    AllowedStatistics = PropertyTypeDescriptor(
        _Job.AllowedStatistics, "AllowedStatistics"
    )
    ColumnSelector = PropertyTypeDescriptor(_Job.ColumnSelector, "ColumnSelector")
    ColumnStatisticsConfiguration = PropertyTypeDescriptor(
        _Job.ColumnStatisticsConfiguration, "ColumnStatisticsConfiguration"
    )
    CsvOutputOptions = PropertyTypeDescriptor(_Job.CsvOutputOptions, "CsvOutputOptions")
    DataCatalogOutput = PropertyTypeDescriptor(
        _Job.DataCatalogOutput, "DataCatalogOutput"
    )
    DatabaseOutput = PropertyTypeDescriptor(_Job.DatabaseOutput, "DatabaseOutput")
    DatabaseTableOutputOptions = PropertyTypeDescriptor(
        _Job.DatabaseTableOutputOptions, "DatabaseTableOutputOptions"
    )
    EntityDetectorConfiguration = PropertyTypeDescriptor(
        _Job.EntityDetectorConfiguration, "EntityDetectorConfiguration"
    )
    JobSample = PropertyTypeDescriptor(_Job.JobSample, "JobSample")
    Output = PropertyTypeDescriptor(_Job.Output, "Output")
    OutputFormatOptions = PropertyTypeDescriptor(
        _Job.OutputFormatOptions, "OutputFormatOptions"
    )
    OutputLocation = PropertyTypeDescriptor(_Job.OutputLocation, "OutputLocation")
    ProfileConfiguration = PropertyTypeDescriptor(
        _Job.ProfileConfiguration, "ProfileConfiguration"
    )
    Recipe = PropertyTypeDescriptor(_Job.Recipe, "Recipe")
    S3Location = PropertyTypeDescriptor(_Job.S3Location, "S3Location")
    S3TableOutputOptions = PropertyTypeDescriptor(
        _Job.S3TableOutputOptions, "S3TableOutputOptions"
    )
    StatisticOverride = PropertyTypeDescriptor(
        _Job.StatisticOverride, "StatisticOverride"
    )
    StatisticsConfiguration = PropertyTypeDescriptor(
        _Job.StatisticsConfiguration, "StatisticsConfiguration"
    )
    ValidationConfiguration = PropertyTypeDescriptor(
        _Job.ValidationConfiguration, "ValidationConfiguration"
    )


@dataclass
class Project(CloudFormationResource):
    """Project resource.

    http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-project.html

    CloudFormation type: AWS::DataBrew::Project
    """

    _resource_type: ClassVar[str] = "AWS::DataBrew::Project"

    dataset_name: DslValue[str] | None = None
    name: DslValue[str] | None = None
    recipe_name: DslValue[str] | None = None
    role_arn: DslValue[str] | None = None
    sample: DslValue[_Project.Sample] | None = None
    tags: list[DslValue[Tag]] = field(default_factory=list)

    # PropertyType descriptors for nested GetAtt support
    Sample = PropertyTypeDescriptor(_Project.Sample, "Sample")


@dataclass
class Recipe(CloudFormationResource):
    """Recipe resource.

    http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-recipe.html

    CloudFormation type: AWS::DataBrew::Recipe
    """

    _resource_type: ClassVar[str] = "AWS::DataBrew::Recipe"

    name: DslValue[str] | None = None
    steps: list[DslValue[_Recipe.RecipeStep]] = field(default_factory=list)
    description: DslValue[str] | None = None
    tags: list[DslValue[Tag]] = field(default_factory=list)

    # PropertyType descriptors for nested GetAtt support
    Action = PropertyTypeDescriptor(_Recipe.Action, "Action")
    ConditionExpression = PropertyTypeDescriptor(
        _Recipe.ConditionExpression, "ConditionExpression"
    )
    DataCatalogInputDefinition = PropertyTypeDescriptor(
        _Recipe.DataCatalogInputDefinition, "DataCatalogInputDefinition"
    )
    Input = PropertyTypeDescriptor(_Recipe.Input, "Input")
    RecipeParameters = PropertyTypeDescriptor(
        _Recipe.RecipeParameters, "RecipeParameters"
    )
    RecipeStep = PropertyTypeDescriptor(_Recipe.RecipeStep, "RecipeStep")
    S3Location = PropertyTypeDescriptor(_Recipe.S3Location, "S3Location")
    SecondaryInput = PropertyTypeDescriptor(_Recipe.SecondaryInput, "SecondaryInput")


@dataclass
class Ruleset(CloudFormationResource):
    """Ruleset resource.

    http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-ruleset.html

    CloudFormation type: AWS::DataBrew::Ruleset
    """

    _resource_type: ClassVar[str] = "AWS::DataBrew::Ruleset"

    name: DslValue[str] | None = None
    rules: list[DslValue[_Ruleset.Rule]] = field(default_factory=list)
    target_arn: DslValue[str] | None = None
    description: DslValue[str] | None = None
    tags: list[DslValue[Tag]] = field(default_factory=list)

    # PropertyType descriptors for nested GetAtt support
    ColumnSelector = PropertyTypeDescriptor(_Ruleset.ColumnSelector, "ColumnSelector")
    Rule = PropertyTypeDescriptor(_Ruleset.Rule, "Rule")
    SubstitutionValue = PropertyTypeDescriptor(
        _Ruleset.SubstitutionValue, "SubstitutionValue"
    )
    Threshold = PropertyTypeDescriptor(_Ruleset.Threshold, "Threshold")


@dataclass
class Schedule(CloudFormationResource):
    """Schedule resource.

    http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-databrew-schedule.html

    CloudFormation type: AWS::DataBrew::Schedule
    """

    _resource_type: ClassVar[str] = "AWS::DataBrew::Schedule"

    cron_expression: DslValue[str] | None = None
    name: DslValue[str] | None = None
    job_names: list[DslValue[str]] = field(default_factory=list)
    tags: list[DslValue[Tag]] = field(default_factory=list)
